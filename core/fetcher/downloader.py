"""
æ•°æ®ä¸‹è½½å™¨ - æ ¸å¿ƒETLæ¨¡å—
æ”¯æŒåˆ†æ®µä¸‹è½½ã€è‡ªåŠ¨åˆå¹¶ã€æ–­ç‚¹ç»­ä¼ 
"""

import os
import pandas as pd
from datetime import datetime, timedelta
from tqdm import tqdm
import logging
import time
from dotenv import load_dotenv

# ç›´æ¥å¯¼å…¥xtquantï¼ˆå·²å¤åˆ¶åˆ°é¡¹ç›®ç¯å¢ƒï¼‰
from xtquant import xtdata

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class QmtDataDownloader:
    """QMTæ•°æ®ä¸‹è½½å™¨"""
    
    def __init__(self, output_dir: str | None = None):
        """åˆå§‹åŒ–ä¸‹è½½å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸ºQmtDataTool/outputæˆ–ä».envè¯»å–
        """
        # åŠ è½½ç¯å¢ƒå˜é‡
        load_dotenv()
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if output_dir is None:
            # ä¼˜å…ˆä»ç¯å¢ƒå˜é‡è¯»å–
            env_output_dir = os.getenv('OUTPUT_DIR')
            if env_output_dir:
                self.output_dir = env_output_dir
                logger.info(f"ğŸ“ ä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®çš„è¾“å‡ºç›®å½•: {self.output_dir}")
            else:
                # ä½¿ç”¨é»˜è®¤ç›®å½•
                current_dir = os.path.dirname(os.path.abspath(__file__))
                qmt_root = os.path.dirname(os.path.dirname(current_dir))
                self.output_dir = os.path.join(qmt_root, 'output')
        else:
            self.output_dir = output_dir
            
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(self.output_dir, exist_ok=True)
        
        logger.info("âœ… æ•°æ®ä¸‹è½½å™¨åˆå§‹åŒ–æˆåŠŸ")
    
    def _generate_time_segments(self, start_time: str, end_time: str | None = None, 
                               years_per_segment: int = 3) -> list[tuple[str, str]]:
        """ç”Ÿæˆæ—¶é—´åˆ†æ®µ
        
        Args:
            start_time: èµ·å§‹æ—¶é—´ï¼Œæ ¼å¼YYYYMMDD
            end_time: ç»“æŸæ—¶é—´ï¼Œæ ¼å¼YYYYMMDDï¼Œé»˜è®¤ä¸ºä»Šå¤©
            years_per_segment: æ¯ä¸ªåˆ†æ®µçš„å¹´æ•°
            
        Returns:
            æ—¶é—´æ®µåˆ—è¡¨ [(start1, end1), (start2, end2), ...]
        """
        if end_time is None:
            end_time = datetime.now().strftime('%Y%m%d')
        
        # è§£ææ—¥æœŸ
        start_dt = datetime.strptime(start_time, '%Y%m%d')
        end_dt = datetime.strptime(end_time, '%Y%m%d')
        
        segments = []
        current_start = start_dt
        
        while current_start < end_dt:
            # è®¡ç®—å½“å‰æ®µçš„ç»“æŸæ—¶é—´
            current_end = datetime(
                current_start.year + years_per_segment,
                current_start.month,
                current_start.day
            )
            
            # å¦‚æœè¶…è¿‡äº†æ€»ç»“æŸæ—¶é—´ï¼Œåˆ™ä½¿ç”¨æ€»ç»“æŸæ—¶é—´
            if current_end > end_dt:
                current_end = end_dt
            
            segments.append((
                current_start.strftime('%Y%m%d'),
                current_end.strftime('%Y%m%d')
            ))
            
            # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªåˆ†æ®µçš„èµ·å§‹æ—¶é—´ï¼ˆå½“å‰ç»“æŸæ—¶é—´+1å¤©ï¼‰
            current_start = current_end + timedelta(days=1)
        
        return segments
    
    def _download_segment(self, code: str, start_time: str, end_time: str,
                         period: str = '1d', dividend_type: str = 'front') -> pd.DataFrame | None:
        """ä¸‹è½½ä¸€ä¸ªæ—¶é—´æ®µçš„æ•°æ®
        
        Args:
            code: è‚¡ç¥¨/ETFä»£ç 
            start_time: èµ·å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            period: å‘¨æœŸï¼Œé»˜è®¤æ—¥çº¿
            dividend_type: å¤æƒæ–¹å¼ï¼Œé»˜è®¤å‰å¤æƒ
            
        Returns:
            DataFrameæˆ–Noneï¼ˆå¦‚æœå¤±è´¥ï¼‰
        """
        try:
            # ç¬¬ä¸€æ­¥ï¼šä¸‹è½½å†å²æ•°æ®åˆ°æœ¬åœ°ç¼“å­˜
            # è¿™æ˜¯QMTçš„å¿…è¦æ­¥éª¤ï¼Œå¿…é¡»å…ˆä¸‹è½½æ•°æ®
            logger.info(f"   ä¸‹è½½ {code} ({start_time} - {end_time}) åˆ°æœ¬åœ°ç¼“å­˜...")
            xtdata.download_history_data(
                stock_code=code,
                period=period,
                start_time=start_time,
                end_time=end_time
            )
            
            # è·å–æ•°æ®å­—æ®µ
            field_list = ['open', 'high', 'low', 'close', 'volume', 'amount']
            
            # ç¬¬äºŒæ­¥ï¼šä»æœ¬åœ°ç¼“å­˜è·å–æ•°æ®
            data_dict = xtdata.get_market_data(
                field_list=field_list,
                stock_list=[code],
                period=period,
                start_time=start_time,
                end_time=end_time,
                dividend_type=dividend_type,
                fill_data=False  # ä¸å¡«å……æ•°æ®
            )
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
            if not data_dict or 'close' not in data_dict:
                logger.warning(f"âš ï¸ {code} åœ¨ {start_time}-{end_time} æœŸé—´æ— æ•°æ®")
                return None
            
            # å°†æ•°æ®å­—å…¸è½¬æ¢ä¸ºDataFrame
            # æ•°æ®æ ¼å¼: {field: DataFrame(index=codes, columns=times)}
            df_list = []
            for field in field_list:
                if field in data_dict:
                    # è½¬ç½®ä½¿æ—¶é—´æˆä¸ºindex
                    df_field = data_dict[field].T
                    # é‡å‘½ååˆ—
                    df_field.columns = [field]
                    df_list.append(df_field)
            
            if not df_list:
                return None
            
            # åˆå¹¶æ‰€æœ‰å­—æ®µ
            df = pd.concat(df_list, axis=1)
            
            # ç¡®ä¿ç´¢å¼•æ˜¯datetimeç±»å‹
            df.index = pd.to_datetime(df.index)
            df.index.name = 'date'
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½ {code} æ•°æ®å¤±è´¥ ({start_time}-{end_time}): {e}")
            return None
    
    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—æ•°æ®
        
        Args:
            df: åŸå§‹æ•°æ®DataFrame
            
        Returns:
            æ¸…æ´—åçš„DataFrame
        """
        if df is None or len(df) == 0:
            return df
        
        # å»é™¤volume=0çš„è¡Œï¼ˆåœç‰Œï¼‰
        df = df[df['volume'] > 0]
        
        # å»é™¤close=0æˆ–NaNçš„è¡Œ
        df = df[(df['close'] > 0) & (df['close'].notna())]
        
        # å»é™¤å…¶ä»–å­—æ®µçš„NaN
        df = df.dropna()
        
        # æŒ‰æ—¥æœŸæ’åº
        df = df.sort_index()
        
        # å»é‡ï¼ˆä¿ç•™æœ€åä¸€æ¡ï¼‰
        df = df[~df.index.duplicated(keep='last')]
        
        return df
    
    def download_stock_data(self, code: str, start_time: str = '20000101',
                           end_time: str = None, period: str = '1d',
                           dividend_type: str = 'front',
                           years_per_segment: int = 3,
                           retry_times: int = 3,
                           output_formats: list[str] | None = None) -> bool:
        """ä¸‹è½½å•ä¸ªè‚¡ç¥¨/ETFçš„å†å²æ•°æ®
        
        Args:
            code: è‚¡ç¥¨/ETFä»£ç 
            start_time: èµ·å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´ï¼Œé»˜è®¤ä»Šå¤©
            period: å‘¨æœŸ
            dividend_type: å¤æƒæ–¹å¼
            years_per_segment: æ¯æ®µçš„å¹´æ•°
            retry_times: é‡è¯•æ¬¡æ•°
            output_formats: è¾“å‡ºæ ¼å¼åˆ—è¡¨ï¼Œå¯é€‰ ['parquet', 'csv', 'excel']
                          é»˜è®¤åªä¿å­˜parquetæ ¼å¼
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        # é»˜è®¤åªä¿å­˜parquetæ ¼å¼
        if output_formats is None:
            output_formats = ['parquet']
        
        logger.info(f"ğŸ“Š å¼€å§‹ä¸‹è½½ {code} çš„æ•°æ®")
        
        # ç”Ÿæˆæ—¶é—´åˆ†æ®µ
        segments = self._generate_time_segments(start_time, end_time, years_per_segment)
        logger.info(f"   åˆ†ä¸º {len(segments)} ä¸ªæ—¶é—´æ®µ")
        
        # å­˜å‚¨æ‰€æœ‰åˆ†æ®µçš„æ•°æ®
        all_data = []
        
        # é€æ®µä¸‹è½½
        for start, end in tqdm(segments, desc=f"ä¸‹è½½{code}"):
            # å°è¯•ä¸‹è½½
            df_segment = None
            for attempt in range(retry_times):
                df_segment = self._download_segment(
                    code, start, end, period, dividend_type
                )
                if df_segment is not None:
                    break
                if attempt < retry_times - 1:
                    logger.warning(f"âš ï¸ é‡è¯• {attempt + 1}/{retry_times}")
                    time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
            
            if df_segment is not None and len(df_segment) > 0:
                all_data.append(df_segment)
        
        if not all_data:
            logger.error(f"âŒ {code} æ²¡æœ‰ä¸‹è½½åˆ°ä»»ä½•æ•°æ®")
            return False
        
        # åˆå¹¶æ‰€æœ‰åˆ†æ®µ
        logger.info(f"   åˆå¹¶ {len(all_data)} ä¸ªæ•°æ®æ®µ")
        df_combined = pd.concat(all_data, axis=0)
        
        # æ¸…æ´—æ•°æ®
        logger.info(f"   æ¸…æ´—æ•°æ®ï¼ˆåŸå§‹è¡Œæ•°: {len(df_combined)}ï¼‰")
        df_clean = self._clean_data(df_combined)
        logger.info(f"   æ¸…æ´—åè¡Œæ•°: {len(df_clean)}ï¼‰")
        
        if len(df_clean) == 0:
            logger.error(f"âŒ {code} æ¸…æ´—åæ— æ•°æ®")
            return False
        
        # ä¿å­˜ä¸ºå¤šç§æ ¼å¼
        saved_files = []
        
        for fmt in output_formats:
            fmt = fmt.lower()
            
            if fmt == 'parquet':
                output_path = os.path.join(self.output_dir, f"{code}.parquet")
                df_clean.to_parquet(output_path, engine='pyarrow', compression='snappy')
                saved_files.append(output_path)
                
            elif fmt == 'csv':
                output_path = os.path.join(self.output_dir, f"{code}.csv")
                df_clean.to_csv(output_path, encoding='utf-8-sig')  # utf-8-sig æ”¯æŒä¸­æ–‡Excelæ‰“å¼€
                saved_files.append(output_path)
                
            elif fmt == 'excel' or fmt == 'xlsx':
                output_path = os.path.join(self.output_dir, f"{code}.xlsx")
                df_clean.to_excel(output_path, engine='openpyxl')
                saved_files.append(output_path)
                
            else:
                logger.warning(f"âš ï¸ ä¸æ”¯æŒçš„æ ¼å¼: {fmt}ï¼Œå·²è·³è¿‡")
        
        # æ‰“å°ä¿å­˜ä¿¡æ¯
        logger.info(f"âœ… {code} æ•°æ®å·²ä¿å­˜")
        logger.info(f"   æ—¶é—´èŒƒå›´: {df_clean.index[0]} ~ {df_clean.index[-1]}")
        logger.info(f"   æ€»è¡Œæ•°: {len(df_clean)}")
        logger.info(f"   ä¿å­˜æ ¼å¼: {', '.join(output_formats)}")
        for file in saved_files:
            logger.info(f"   æ–‡ä»¶: {file}")
        
        return True
    
    def download_batch(self, code_list: list[str], **kwargs) -> dict[str, bool]:
        """æ‰¹é‡ä¸‹è½½å¤šä¸ªè‚¡ç¥¨/ETFçš„æ•°æ®
        
        Args:
            code_list: ä»£ç åˆ—è¡¨
            **kwargs: ä¼ é€’ç»™download_stock_dataçš„å…¶ä»–å‚æ•°
            
        Returns:
            ä¸‹è½½ç»“æœå­—å…¸ {code: success}
        """
        results = {}
        
        logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡ä¸‹è½½ {len(code_list)} ä¸ªæ ‡çš„")
        
        for code in code_list:
            success = self.download_stock_data(code, **kwargs)
            results[code] = success
            # æ¯ä¸ªæ ‡çš„ä¹‹é—´æš‚åœä¸€ä¸‹ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(0.5)
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for v in results.values() if v)
        logger.info(f"ğŸ“ˆ æ‰¹é‡ä¸‹è½½å®Œæˆ: {success_count}/{len(code_list)} æˆåŠŸ")
        
        # ä¿å­˜æˆåŠŸä¸‹è½½çš„è‚¡ç¥¨åˆ—è¡¨
        if success_count > 0:
            self.save_stock_list(results)
        
        return results
    
    def save_stock_list(self, results: dict[str, bool]) -> None:
        """ä¿å­˜å·²ä¸‹è½½çš„è‚¡ç¥¨ä»£ç åˆ—è¡¨
        
        Args:
            results: ä¸‹è½½ç»“æœå­—å…¸ {code: success}
        """
        # ç­›é€‰æˆåŠŸçš„è‚¡ç¥¨
        successful_codes = [code for code, success in results.items() if success]
        
        if not successful_codes:
            return
        
        # åˆ›å»ºDataFrame
        stock_list_data = []
        for code in successful_codes:
            # å°è¯•è¯»å–æ•°æ®è·å–è¯¦ç»†ä¿¡æ¯
            try:
                parquet_file = os.path.join(self.output_dir, f"{code}.parquet")
                if os.path.exists(parquet_file):
                    df = pd.read_parquet(parquet_file)
                    stock_list_data.append({
                        'ä»£ç ': code,
                        'èµ·å§‹æ—¥æœŸ': str(df.index[0].date()),
                        'ç»“æŸæ—¥æœŸ': str(df.index[-1].date()),
                        'æ•°æ®é‡': len(df),
                        'æ–‡ä»¶': f"{code}.parquet"
                    })
                else:
                    stock_list_data.append({
                        'ä»£ç ': code,
                        'èµ·å§‹æ—¥æœŸ': '-',
                        'ç»“æŸæ—¥æœŸ': '-',
                        'æ•°æ®é‡': 0,
                        'æ–‡ä»¶': '-'
                    })
            except Exception as e:
                logger.warning(f"âš ï¸ è¯»å– {code} ä¿¡æ¯å¤±è´¥: {e}")
                stock_list_data.append({
                    'ä»£ç ': code,
                    'èµ·å§‹æ—¥æœŸ': '-',
                    'ç»“æŸæ—¥æœŸ': '-',
                    'æ•°æ®é‡': 0,
                    'æ–‡ä»¶': '-'
                })
        
        df_list = pd.DataFrame(stock_list_data)
        
        # ä¿å­˜ä¸ºCSVï¼ˆæ–¹ä¾¿æŸ¥çœ‹ï¼‰
        csv_path = os.path.join(self.output_dir, 'stock_list.csv')
        df_list.to_csv(csv_path, index=False, encoding='utf-8-sig')
        logger.info(f"ğŸ“‹ è‚¡ç¥¨åˆ—è¡¨å·²ä¿å­˜åˆ°: {csv_path}")
        
        # ä¿å­˜ä¸ºExcelï¼ˆæ›´ç¾è§‚ï¼‰
        try:
            excel_path = os.path.join(self.output_dir, 'stock_list.xlsx')
            df_list.to_excel(excel_path, index=False, engine='openpyxl')
            logger.info(f"ğŸ“‹ è‚¡ç¥¨åˆ—è¡¨å·²ä¿å­˜åˆ°: {excel_path}")
        except Exception as e:
            logger.warning(f"âš ï¸ ä¿å­˜Excelå¤±è´¥: {e}ï¼Œè¯·å®‰è£…openpyxl: pip install openpyxl")


def load_data(code: str, output_dir: str = None) -> pd.DataFrame:
    """ä»outputç›®å½•è¯»å–æŒ‡å®šä»£ç çš„æ•°æ®
    
    Args:
        code: è‚¡ç¥¨/ETFä»£ç 
        output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸ºé¡¹ç›®å†…çš„outputç›®å½•
        
    Returns:
        DataFrame
    """
    if output_dir is None:
        # è·å–é»˜è®¤outputç›®å½•
        current_dir = os.path.dirname(os.path.abspath(__file__))
        qmt_root = os.path.dirname(os.path.dirname(current_dir))
        output_dir = os.path.join(qmt_root, 'output')
    
    file_path = os.path.join(output_dir, f"{code}.parquet")
    
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    df = pd.read_parquet(file_path, engine='pyarrow')
    return df
